[
  {
    "text": "Jul 25, 2024                       • Chip Huyen  After studying how companies deploy generative AI applications, I noticed many similarities in their platforms. This post outlines the common components of a generative AI platform, what they do, and how they are implemented. I try my best to keep the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "architecture general, but certain applications might deviate. This is what the overall architecture looks like.  This is a pretty complex system. This post will start from the simplest architecture and progressively add more components. In its simplest form, your application receives a query and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "sends it to the model. The model generates a response, which is returned to the user. There are no guardrails, no augmented context, and no optimization. The Model API box refers to both third-party APIs (e.g., OpenAI, Google, Anthropic) and self-hosted APIs.  From this, you can add more components",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "as needs arise. The order discussed in this post is common, though you don’t need to follow the exact same order. A component can be skipped if your system works well without it. Evaluation is necessary at every step of the development process. Observability, which allows you to gain visibility into",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "your system for monitoring and debugging, and orchestration, which involves chaining all the components together, are two essential components of the platform. We will discuss them at the end of this post. » What this post is not « This post focuses on the overall architecture for deploying AI",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "applications. It discusses what components are needed and considerations when building these components. It’s not about how to build AI applications and, therefore, does NOT discuss model evaluation, application evaluation, prompt engineering, finetuning, data annotation guidelines, or chunking",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "strategies for RAGs. All these topics are covered in my upcoming book AI Engineering.  The initial expansion of a platform usually involves adding mechanisms to allow the system to augment each query with the necessary information. Gathering the relevant information is called context construction.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Many queries require context to answer. The more relevant information there is in the context, the less the model has to rely on its internal knowledge, which can be unreliable due to its training data and training methodology. Studies have shown that having access to relevant information in the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "context can help the model generate more detailed responses while reducing hallucinations (Lewis et al., 2020). For example, given the query “Will Acme’s fancy-printer-A300 print 100pps?”, the model will be able to respond better if it’s given the specifications of fancy-printer-A300. (Thanks Chetan",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Tekur for the example.) Context construction for foundation models is equivalent to feature engineering for classical ML models. They serve the same purpose: giving the model the necessary information to process an input. In-context learning, learning from the context, is a form of continual",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "learning. It enables a model to incorporate new information continually to make decisions, preventing it from becoming outdated. For example, a model trained on last-week data won’t be able to answer questions about this week unless the new information is included in its context. By updating a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model’s context with the latest information, e.g. fancy-printer-A300’s latest specifications, the model remains up-to-date and can respond to queries beyond its cut-off date. The most well-known pattern for context construction is RAG, Retrieval-Augmented Generation. RAG consists of two components:",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "a generator (e.g. a language model) and a retriever, which retrieves relevant information from external sources.  Retrieval isn’t unique to RAGs. It’s the backbone of search engines, recommender systems, log analytics, etc. Many retrieval algorithms developed for traditional retrieval systems can be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "used for RAGs. External memory sources typically contain unstructured data, such as memos, contracts, news updates, etc. They can be collectively called documents. A document can be 10 tokens or 1 million tokens. Naively retrieving whole documents can cause your context to be arbitrarily long. RAG",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "typically requires documents to be split into manageable chunks, which can be determined from the model’s maximum context length and your application’s latency requirements. To learn more about chunking and the optimal chunk size, see Pinecone, Langchain, Llamaindex, and Greg Kamradt’s tutorials.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Once data from external memory sources has been loaded and chunked, retrieval is performed using two main approaches. Embedding-based retrieval (also known as vector search)  You convert chunks of data into embedding vectors using an embedding model such as BERT, sentence-transformers, and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "proprietary embedding models provided by OpenAI or Google. Given a query, the data whose vectors are closest to the query embedding, as determined by the vector search algorithm, is retrieved.   Vector search is usually framed as nearest-neighbor search, using approximate nearest neighbor (ANN)",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "algorithms such as FAISS (Facebook AI Similarity Search), Google’s ScaNN, Spotify’s ANNOY, and hnswlib (Hierarchical Navigable Small World).    The ANN-benchmarks website compares different ANN algorithms on multiple datasets using four main metrics, taking into account the tradeoffs between",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "indexing and querying. This works with not just text documents, but also images, videos, audio, and code. Many teams even try to summarize SQL tables and dataframes and then use these summaries to generate embeddings for retrieval. Term-based retrieval is much faster and cheaper than embedding-based",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "retrieval. It can work well out of the box, making it an attractive option to start. Both BM25 and Elasticsearch are widely used in the industry and serve as formidable baselines for more complex retrieval systems. Embedding-based retrieval, while computationally expensive, can be significantly",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "improved over time to outperform term-based retrieval. A production retrieval system typically combines several approaches. Combining term-based retrieval and embedding-based retrieval is called hybrid search. One common pattern is sequential. First, a cheap, less precise retriever, such as a term-",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "based system, fetches candidates. Then, a more precise but more expensive mechanism, such as k-nearest neighbors, finds the best of these candidates. The second step is also called reranking. For example, given the term “transformer”, you can fetch all documents that contain the word transformer,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "regardless of whether they are about the electric device, the neural architecture, or the movie. Then you use vector search to find among these documents those that are actually related to your transformer query. Context reranking differs from traditional search reranking in that the exact position",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "of items is less critical. In search, the rank (e.g., first or fifth) is crucial. In context reranking, the order of documents still matters because it affects how well a model can process them. Models might better understand documents at the beginning and end of the context, as suggested by the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "paper Lost in the middle (Liu et al., 2023). However, as long as a document is included, the impact of its order is less significant compared to in search ranking. Another pattern is ensemble. Remember that a retriever works by ranking documents by their relevance scores to the query. You use",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "multiple retrievers to fetch candidates at the same time, then combine these different rankings together to generate a final ranking. External data sources can also be structured, such as dataframes or SQL tables. Retrieving data from an SQL table is significantly different from retrieving data from",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "unstructured documents. Given a query, the system works as follows.  For the text-to-SQL step, if there are many available tables whose schemas can’t all fit into the model context, you might need an intermediate step to predict what tables to use for each query. Text-to-SQL can be done by the same",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model used to generate the final response or one of many specialized text-to-SQL models. An important source of data is the Internet. A web search tool like Google or Bing API can give the model access to a rich, up-to-date resource to gather relevant information for each query. For example, given",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "the query “Who won Oscar this year?”, the system searches for information about the latest Oscar and uses this information to generate the final response to the user. Term-based retrieval, embedding-based retrieval, SQL execution, and web search are actions that a model can take to augment its",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "context. You can think of each action as a function the model can call. A workflow that can incorporate external actions is also called agentic. The architecture then looks like this.  » Action vs. tool « A tool allows one or more actions. For example, a people search tool might allow two actions:",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "search by name and search by email. However, the difference is minimal, so many people use action and tool interchangeably. » Read-only actions vs. write actions « Actions that retrieve information from external sources but don’t change their states are read-only actions. Giving a model write",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "actions, e.g. updating the values in a table, enables the model to perform more tasks but also poses more risks, which will be discussed later. Often, a user query needs to be rewritten to increase the likelihood of fetching the right information. Consider the following conversation. The last",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "question, “How about Emily Doe?”, is ambiguous. If you use this query verbatim to retrieve documents, you’ll likely get irrelevant results. You need to rewrite this query to reflect what the user is actually asking. The new query should make sense on its own. The last question should be rewritten to",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "“When was the last time Emily Doe bought something from us?” Query rewriting is typically done using other AI models, using a prompt similar to “Given the following conversation, rewrite the last user input to reflect what the user is actually asking.”  Query rewriting can get complicated,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "especially if you need to do identity resolution or incorporate other knowledge. If the user asks “How about his wife?”, you will first need to query your database to find out who his wife is. If you don’t have this information, the rewriting model should acknowledge that this query isn’t solvable",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "instead of hallucinating a name, leading to a wrong answer. Guardrails help reduce AI risks and protect not just your users but also you, the developers. They should be placed whenever there is potential for failures. This post discusses two types of guardrails: input guardrails and output",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "guardrails. Input guardrails are typically protection against two types of risks: leaking private information to external APIs, and executing bad prompts that compromise your system (model jailbreaking). This risk is specific to using external model APIs when you need to send your data outside your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "organization. For example, an employee might copy the company’s secret or a user’s private information into a prompt and send it to wherever the model is hosted.   One of the most notable early incidents was when Samsung employees put Samsung’s proprietary information into ChatGPT, accidentally",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "leaking the company’s secrets. It’s unclear how Samsung discovered this leak and how the leaked information was used against Samsung. However, the incident was serious enough for Samsung to ban ChatGPT in May 2023.   There’s no airtight way to eliminate potential leaks when using third-party APIs.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "However, you can mitigate them with guardrails. You can use one of the many available tools that automatically detect sensitive data. What sensitive data to detect is specified by you. Common sensitive data classes are: Many sensitive data detection tools use AI to identify potentially sensitive",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "information, such as determining if a string resembles a valid home address. If a query is found to contain sensitive information, you have two options: block the entire query or remove the sensitive information from it. For instance, you can mask a user’s phone number with the placeholder [PHONE",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "NUMBER]. If the generated response contains this placeholder, use a PII reversible dictionary that maps this placeholder to the original information so that you can unmask it, as shown below.  It’s become an online sport to try to jailbreak AI models, getting them to say or do bad things. While some",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "might find it amusing to get ChatGPT to make controversial statements, it’s much less fun if your customer support chatbot, branded with your name and logo, does the same thing. This can be especially dangerous for AI systems that have access to tools. Imagine if a user finds a way to get your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "system to execute an SQL query that corrupts your data.   To combat this, you should first put guardrails on your system so that no harmful actions can be automatically executed. For example, no SQL queries that can insert, delete, or update data can be executed without human approval. The downside",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "of this added security is that it can slow down your system. To prevent your application from making outrageous statements it shouldn’t be making, you can define out-of-scope topics for your application. For example, if your application is a customer support chatbot, it shouldn’t answer political or",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "social questions. A simple way to do so is to filter out inputs that contain predefined phrases typically associated with controversial topics, such as “immigration” or “antivax”. More sophisticated algorithms use AI to classify whether an input is about one of the pre-defined restricted topics.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "If harmful prompts are rare in your system, you can use an anomaly detection algorithm to identify unusual prompts. AI models are probabilistic, making their outputs unreliable. You can put in guardrails to significantly improve your application’s reliability. Output guardrails have two main",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "functionalities: To catch outputs that fail to meet your standards, you need to understand what failures look like. Here are examples of failure modes and how to catch them. Empty responses. Malformatted responses that don’t follow the expected output format. For example, if the application expects",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "JSON and the generated response has a missing closing bracket. There are validators for certain formats, such as regex, JSON, and Python code validators. There are also tools for constrained sampling such as guidance, outlines, and instructor. Toxic responses, such as those that are racist or",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "sexist. These responses can be caught using one of many toxicity detection tools. Factual inconsistent responses hallucinated by the model. Hallucination detection is an active area of research with solutions such as SelfCheckGPT (Manakul et al., 2023) and SAFE, Search Engine Factuality Evaluator",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "(Wei et al., 2024). You can mitigate hallucinations by providing models with sufficient context and prompting techniques such as chain-of-thought. Hallucination detection and mitigation are discussed further in my upcoming book AI Engineering. This failure mode can be prevented by not training your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model on sensitive data and not allowing it to retrieve sensitive data in the first place. Sensitive data in outputs can be detected using the same tools used for input guardrails. Brand-risk responses, such as responses that mischaracterize your company or your competitors. An example is when Grok,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "a model trained by X, generated a response suggesting that Grok was trained by OpenAI, causing the Internet to suspect X of stealing OpenAI’s data. This failure mode can be mitigated with keyword monitoring. Once you’ve identified outputs concerning your brands and competitors, you can either block",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "these outputs, pass them onto human reviewers, or use other models to detect the sentiment of these outputs to ensure that only the right sentiments are returned. AI models are probabilistic, which means that if you try a query again, you might get a different response. Many failures can be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "mitigated using a basic retry logic. For example, if the response is empty, try again X times or until you get a non-empty response. Similarly, if the response is malformatted, try again until the model generates a correctly formatted response. This retry policy, however, can incur extra latency and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "cost. One retry means 2x the number of API calls. If the retry is carried out after failure, the latency experienced by the user will double. To reduce latency, you can make calls in parallel. For example, for each query, instead of waiting for the first query to fail before retrying, you send this",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "query to the model twice at the same time, get back two responses, and pick the better one. This increases the number of redundant API calls but keeps latency manageable. It’s also common to fall back on humans to handle tricky queries. For example, you can transfer a query to human operators if it",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "contains specific key phrases. Some teams use a specialized model, potentially trained in-house, to decide when to transfer a conversation to humans. One team, for instance, transfers a conversation to human operators when their sentiment analysis model detects that the user is getting angry.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Another team transfers a conversation after a certain number of turns to prevent users from getting stuck in an infinite loop. Reliability vs. latency tradeoff: While acknowledging the importance of guardrails, some teams told me that latency is more important. They decided not to implement",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "guardrails because they can significantly increase their application’s latency. However, these teams are in the minority. Most teams find that the increased risks are more costly than the added latency. Output guardrails might not work well in the stream completion mode. By default, the whole",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "response is generated before shown to the user, which can take a long time. In the stream completion mode, new tokens are streamed to the user as they are generated, reducing the time the user has to wait to see the response. The downside is that it’s hard to evaluate partial responses, so unsafe",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "responses might be streamed to users before the system guardrails can determine that they should be blocked. Self-hosted vs. third-party API tradeoff: Self-hosting your models means that you don’t have to send your data to a third party, reducing the need for input guardrails. However, it also means",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "that you must implement all the necessary guardrails yourself, rather than relying on the guardrails provided by third-party services. Our platform now looks like this. Guardrails can be independent tools or parts of model gateways, as discussed later. Scorers, if used, are grouped under model APIs",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "since scorers are typically AI models, too. Models used for scoring are typically smaller and faster than models used for generation.  As applications grow in complexity and involve more models, two types of tools emerged to help you work with multiple models: routers and gateways. An application",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "can use different models to respond to different types of queries. Having different solutions for different queries has several benefits. First, this allows you to have specialized solutions, such as one model specialized in technical troubleshooting and another specialized in subscriptions.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Specialized models can potentially perform better than a general-purpose model. Second, this can help you save costs. Instead of routing all queries to an expensive model, you can route simpler queries to cheaper models. A router typically consists of an intent classifier that predicts what the user",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "is trying to do. Based on the predicted intent, the query is routed to the appropriate solution. For example, for a customer support chatbot, if the intent is: An intent classifier can also help your system avoid out-of-scope conversations. For example, you can have an intent classifier that",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "predicts whether a query is out of the scope. If the query is deemed inappropriate (e.g. if the user asks who you would vote for in the upcoming election), the chatbot can politely decline to engage using one of the stock responses (“As a chatbot, I don’t have the ability to vote. If you have",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "questions about our products, I’d be happy to help.”) without wasting an API call. If your system has access to multiple actions, a router can involve a next-action predictor to help the system decide what action to take next. One valid action is to ask for clarification if the query is ambiguous.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "For example, in response to the query “Freezing,” the system might ask, “Do you want to freeze your account or are you talking about the weather?” or simply say, “I’m sorry. Can you elaborate?” Intent classifiers and next-action predictors can be general-purpose models or specialized classification",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "models. Specialized classification models are typically much smaller and faster than general-purpose models, allowing your system to use multiple of them without incurring significant extra latency and cost. When routing queries to models with varying context limits, the query’s context might need",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to be adjusted accordingly. Consider a query of 1,000 tokens that is slated for a model with a 4K context limit. The system then takes an action, e.g. web search, that brings back 8,000-token context. You can either truncate the query’s context to fit the originally intended model or route the query",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to a model with a larger context limit. A model gateway is an intermediate layer that allows your organization to interface with different models in a unified and secure manner. The most basic functionality of a model gateway is to enable developers to access different models – be it self-hosted",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "models or models behind commercial APIs such as OpenAI or Google – the same way. A model gateway makes it easier to maintain your code. If a model API changes, you only need to update the model gateway instead of having to update all applications that use this model API.  In its simplest form, a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model gateway is a unified wrapper that looks like the following code example. This example is to give you an idea of how a model gateway might be implemented. It’s not meant to be functional as it doesn’t contain any error checking or optimization. A model gateway is access control and cost",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "management. Instead of giving everyone who wants access to the OpenAI API your organizational tokens, which can be easily leaked, you only give people access to the model gateway, creating a centralized and controlled point of access. The gateway can also implement fine-grained access controls,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "specifying which user or application should have access to which model. Moreover, the gateway can monitor and limit the usage of API calls, preventing abuse and managing costs effectively. A model gateway can also be used to implement fallback policies to overcome rate limits or API failures (the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "latter is unfortunately common). When the primary API is unavailable, the gateway can route requests to alternative models, retry after a short wait, or handle failures in other graceful manners. This ensures that your application can operate smoothly without interruptions. Since requests and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "responses are already flowing through the gateway, it’s a good place to implement other functionalities such as load balancing, logging, and analytics. Some gateway services even provide caching and guardrails. Given that gateways are relatively straightforward to implement, there are many off-the-",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "shelf gateways. Examples include Portkey’s gateway, MLflow AI Gateway, WealthSimple’s llm-gateway, TrueFoundry, Kong, and Cloudflare. With the added gateway and routers, our platform is getting more exciting. Like scoring, routing is also in the model gateway. Like models used for scoring, models",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "used for routing are typically smaller than models used for generation.  When I shared this post with my friend Eugene Yan, he said that cache is perhaps the most underrated component of an AI platform. Caching can significantly reduce your application’s latency and cost. Cache techniques can also",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "be used during training, but since this post is about deployment, I’ll focus on cache for inference. Some common inference caching techniques include prompt cache, exact cache, and semantic cache. Prompt cache are typically implemented by the inference APIs that you use. When evaluating an inference",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "library, it’s helpful to understand what cache mechanism it supports. KV cache for the attention mechanism is out of scope for this discussion. Many prompts in an application have overlapping text segments. For example, all queries can share the same system prompt. A prompt cache stores these",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "overlapping segments for reuse, so you only need to process them once. A common overlapping text segment in different prompts is the system prompt. Without prompt cache, your model needs to process the system prompt with every query. With prompt cache, it only needs to process the system prompt once",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "for the first query. For applications with long system prompts, prompt cache can significantly reduce both latency and cost. If your system prompt is 1000 tokens and your application generates 1 million model API calls today, a prompt cache will save you from processing approximately 1 billion",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "repetitive input tokens a day! However, this isn’t entirely free. Like KV cache, prompt cache size can be quite large and require significant engineering effort. Prompt cache is also useful for queries that involve long documents. For example, if many of your user queries are related to the same",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "long document (such as a book or a codebase), this long document can be cached for reuse across queries. Since its introduction in November 2023 by Gim et al., prompt cache has already been incorporated into model APIs. Google announced that Gemini APIs will offer this functionality in June 2024",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "under the name context cache. Cached input tokens are given a 75% discount compared to regular input tokens, but you’ll have to pay extra for cache storage (as of writing, $1.00 / 1 million tokens per hour). Given the obvious benefits of prompt cache, I wouldn’t be surprised if it becomes as popular",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "as KV cache. While llama.cpp also has prompt cache, it seems to only cache whole prompts and work for queries in the same chat session. Its documentation is limited, but my guess from reading the code is that in a long conversation, it caches the previous messages and only processes the newest",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "message. If prompt cache and KV cache are unique to foundation models, exact cache is more general and straightforward. Your system stores processed items for reuse later when the exact items are requested. For example, if a user asks a model to summarize a product, the system checks the cache to",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "see if a summary of this product is cached. If yes, fetch this summary. If not, summarize the product and cache the summary. Exact cache is also used for embedding-based retrieval to avoid redundant vector search. If an incoming query is already in the vector search cache, fetch the cached search",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "result. If not, perform a vector search for this query and cache the result. Cache is especially appealing for queries that require multiple steps (e.g. chain-of-thought) and/or time-consuming actions (e.g. retrieval, SQL execution, or web search). An exact cache can be implemented using in-memory",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "storage for fast retrieval. However, since in-memory storage is limited, a cache can also be implemented using databases like PostgreSQL, Redis, or tiered storage to balance speed and storage capacity. Having an eviction policy is crucial to manage the cache size and maintain performance. Common",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "eviction policies include Least Recently Used (LRU), Least Frequently Used (LFU), and First In, First Out (FIFO). How long to cache a query depends on how likely this query is to be called again. User-specific queries such as “What’s the status of my recent order” are less likely to be reused by",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "other users, and therefore, shouldn’t be cached. Similarly, it makes less sense to cache time-sensitive queries such as “How’s the weather?” Some teams train a small classifier to predict whether a query should be cached. Unlike exact cache, semantic cache doesn’t require the incoming query to be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "identical to any of the cached queries. Semantic cache allows the reuse of similar queries. Imagine one user asks “What’s the capital of Vietnam?” and the model generates the answer “Hanoi”. Later, another user asks “What’s the capital city of Vietnam?”, which is the same question but with the extra",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "word “city”. The idea of semantic cache is that the system can reuse the answer “Hanoi” instead of computing the new query from scratch. Semantic cache only works if you have a reliable way to determine if two queries are semantically similar. One common approach is embedding-based similarity, which",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "works as follows: This approach requires a vector database to store the embeddings of cached queries. Compared to other caching techniques, semantic cache’s value is more dubious because many of its components are prone to failure. Its success relies on high-quality embeddings, functional vector",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "search, and a trustworthy similarity metric. Setting the right similarity threshold can also be tricky and require a lot of trial and error. If the system mistakes the incoming query as being similar to another query, the returned response, fetched from the cache, will be incorrect. In addition,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "semantic cache can be time-consuming and compute-intensive, as it involves a vector search. The speed and cost of this vector search depend on the size of your database of cached embeddings. Semantic cache might still be worth it if the cache hit rate is high, meaning that a good portion of queries",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "can be effectively answered by leveraging the cached results. However, before incorporating the complexities of semantic cache, make sure to evaluate the efficiency, cost, and performance risks associated with it. With the added cache systems, the platform looks as follows. KV cache and prompt cache",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "are typically implemented by model API providers, so they aren’t shown in this image. If I must visualize them, I’d put them in the Model API box. There’s a new arrow to add generated responses to the cache.  The applications we’ve discussed so far have fairly simple flows. The outputs generated by",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "foundation models are mostly returned to users (unless they don’t pass the guardrails). However, an application flow can be more complex with loops and conditional branching. A model’s outputs can also be used to invoke write actions, such as composing an email or placing an order. Outputs from a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model can be conditionally passed onto another model or fed back to the same model as part of the input to the next step. This goes on until a model in the system decides that the task has been completed and that a final response should be returned to the user. This can happen when you give your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "system the ability to plan and decide what to do next. As an example, consider the query “Plan a weekend itinerary for Paris.” The model might first generate a list of potential activities: visiting the Eiffel Tower, having lunch at a café, touring the Louvre, etc. Each of these activities can then",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "be fed back into the model to generate more detailed plans. For instance, “visiting the Eiffel Tower” could prompt the model to generate sub-tasks like checking the opening hours, buying tickets, and finding nearby restaurants. This iterative process continues until a comprehensive and detailed",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "itinerary is created. Our infrastructure now has an arrow pointing the generated response back to context construction, which in turn feeds back to models in the model gateway.  Actions used for context construction are read-only actions. They allow a model to read from its data sources to gather",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "context. But a system can also write actions, making changes to the data sources and the world. For example, if the model outputs: “send an email to X with the message Y”, the system will invoke the action send_email(recipient=X, message=Y). Write actions make a system vastly more capable. They can",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "enable you to automate the whole customer outreach workflow: researching potential customers, finding their contacts, drafting emails, sending first emails, reading responses, following up, extracting orders, updating your databases with new orders, etc. However, the prospect of giving AI the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "ability to automatically alter our lives is frightening. Just as you shouldn’t give an intern the authority to delete your production database, you shouldn’t allow an unreliable AI to initiate bank transfers. Trust in the system’s capabilities and its security measures is crucial. You need to ensure",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "that the system is protected from bad actors who might try to manipulate it into performing harmful actions. AI systems are vulnerable to cyber attacks like other software systems, but they also have another weakness: prompt injection. Prompt injection happens when an attacker manipulates input",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "prompts into a model to get it to express undesirable behaviors. You can think of prompt injection as social engineering done on AI instead of humans. A scenario that many companies fear is that they give an AI system access to their internal databases, and attackers trick this system into revealing",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "private information from these databases. If the system has write access to these databases, attackers can trick the system into corrupting the data. Any organization that wants to leverage AI needs to take safety and security seriously. However, these risks don’t mean that AI systems should never",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "be given the ability to act in the real world. AI systems can fail, but humans can fail too. If we can get people to trust a machine to take us up into space, I hope that one day, securities will be sufficient for us to trust autonomous AI systems.  While I have placed observability in its own",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "section, it should be integrated into the platform from the beginning rather than added later as an afterthought. Observability is crucial for projects of all sizes, and its importance grows with the complexity of the system. This section provides the least information compared to the others. It’s",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "impossible to cover all the nuances of observability in a blog post. Therefore, I will only give a brief overview of the three pillars of monitoring: logs, traces, and metrics. I won’t go into specifics or cover user feedback, drift detection, and debugging. When discussing monitoring, most people",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "think of metrics. What metrics to track depends on what you want to track about your system, which is application-specific. However, in general, there are two types of metrics you want to track: model metrics and system metrics. System metrics tell you the state of your overall system. Common",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "metrics are throughput, memory usage, hardware utilization, and service availability/uptime. System metrics are common to all software engineering applications. In this post, I’ll focus on model metrics. Model metrics assess your model’s performance, such as accuracy, toxicity, and hallucination",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "rate. Different steps in an application pipeline also have their own metrics. For example, in a RAG application, the retrieval quality is often evaluated using context relevance and context precision. A vector database can be evaluated by how much storage it needs to index the data and how long it",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "takes to query the data There are various ways a model’s output can fail. It’s crucial to identify these issues and develop metrics to monitor them. For example, you might want to track how often your model times out, returns empty responses or produces malformatted responses. If you’re worried",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "about your model revealing sensitive information, find a way to track that too. Length-related metrics such as query, context, and response length are helpful for understanding your model’s behaviors. Is one model more verbose than another? Are certain types of queries more likely to result in",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "lengthy answers? They are especially useful for detecting changes in your application. If the average query length suddenly decreases, it could indicate an underlying issue that needs investigation. Length-related metrics are also important for tracking latency and costs, as longer contexts and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "responses typically increase latency and incur higher costs. Tracking latency is essential for understanding the user experience. Common latency metrics include: You’ll also want to track costs. Cost-related metrics are the number of queries and the volume of input and output tokens. If you use an",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "API with rate limits, tracking the number of requests per second is important to ensure you stay within your allocated limits and avoid potential service interruptions. When calculating metrics, you can choose between spot checks and exhaustive checks. Spot checks involve sampling a subset of data",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to quickly identify issues, while exhaustive checks evaluate every request for a comprehensive performance view. The choice depends on your system’s requirements and available resources, with a combination of both providing a balanced monitoring strategy. When computing metrics, ensure they can be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "broken down by relevant axes, such as users, releases, prompt/chain versions, prompt/chain types, and time. This granularity helps in understanding performance variations and identifying specific issues. Since this blog post is getting long and I’ve written at length about logs in Designing Machine",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Learning Systems, I will be quick here. The philosophy for logging is simple: log everything. Log the system configurations. Log the query, the output, and the intermediate outputs. Log when a component starts, ends, when something crashes, etc. When recording a piece of log, make sure to give it",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "tags and IDs that can help you know where in the system this log comes from. Logging everything means that the amount of logs you have can grow very quickly. Many tools for automated log analysis and log anomaly detection are powered by AI. While it’s impossible to manually process logs, it’s useful",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to manually inspect your production data daily to get a sense of how users are using your application. Shankar et al. (2024) found that the developers’ perceptions of what constitutes good and bad outputs change as they interact with more data, allowing them to both rewrite their prompts to increase",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "the chance of good responses and update their evaluation pipeline to catch bad responses. Trace refers to the detailed recording of a request’s execution path through various system components and services. In an AI application, tracing reveals the entire process from when a user sends a query to",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "when the final response is returned, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. It should also show how much time each step takes and its associated cost, if measurable. As an example, this is a visualization of a Langsmith trace.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Ideally, you should be able to trace each query’s transformation through the system step-by-step. If a query fails, you should be able to pinpoint the exact step where it went wrong: whether it was incorrectly processed, the retrieved context was irrelevant, or the model generated a wrong response.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "An AI application can get fairly complex, consisting of multiple models, retrieving data from many databases, and having access to a wide range of tools. An orchestrator helps you specify how these different components are combined (chained) together to create an end-to-end application flow. At a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "high level, an orchestrator works in two steps: components definition and chaining (also known as pipelining). Components Definition  You need to tell the orchestrator what components your system uses, such as models (including models for generation, routing, and scoring), databases from which your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "system can retrieve data, and actions that your system can take. Direct integration with model gateways can help simplify model onboarding, and some orchestrator tools want to be gateways. Many orchestrators also support integration with tools for evaluation and monitoring. Chaining (or pipelining)",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "You tell the orchestrator the sequence of steps your system takes from receiving the user query until completing the task. In short, chaining is just function composition. Here’s an example of what a pipeline looks like. The orchestrator is responsible for passing data between steps and can provide",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "toolings that help ensure that the output from the current step is in the format expected by the next step. When designing the pipeline for an application with strict latency requirements, try to do as much in parallel as possible. For example, if you have a routing component (deciding where to send",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "a query to) and a PII removal component, they can do both at the same time. There are many AI orchestration tools, including LangChain, LlamaIndex, Flowise, Langflow, and Haystack. Each tool has its own APIs so I won’t show the actual code here. While it’s tempting to jump straight to an",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "orchestration tool when starting a project, start building your application without one first. Any external tool brings added complexity. An orchestrator can abstract away critical details of how your system works, making it hard to understand and debug your system. As you advance to the later",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "stages of your application development process, you might decide that an orchestrator can make your life easier. Here are three aspects to keep in mind when evaluating orchestrators. This post started with a basic architecture and then gradually added components to address the growing application",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "complexities. Each addition brings its own set of benefits and challenges, requiring careful consideration and implementation. While the separation of components is important to keep your system modular and maintainable, this separation is fluid. There are many overlaps between components. For",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "example, a model gateway can share functionalities with guardrails. Cache can be implemented in different components, such as in vector search and inference services. This post is much longer than I intended it to be, and yet there are many details I haven’t been able to explore further, especially",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "around observability, context construction, complex logic, cache, and guardrails. I’ll dive deeper into all these components in my upcoming book AI Engineering. This post also didn’t discuss how to serve models, assuming that most people will be using models provided by third-party APIs. AI",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Engineering will also have a chapter dedicated to inference and model optimization. Special thanks to Luke Metz, Alex Li, Chetan Tekur, Kittipat “Bot” Kampa, Hien Luu, and Denys Linkov for feedback on the early versions of this post. Their insights greatly improved the content. Any remaining errors",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "are my own. I read many case studies shared by companies on how they adopted generative AI, and here are some of my favorites. I help companies deploy machine learning into production. I write about AI applications, tooling, and best practices.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Update: On Aug. 31st 2024, we released the 2nd version of Jina-ColBERT, with improved performance, multilingual support over 89 languages and flexible output dimensions. Check the release post for more details. Last Friday, the release of the ColBERT model by Jina AI on Hugging Face sparked",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "significant excitement across the AI community, particularly on Twitter/X. While many are familiar with the groundbreaking BERT model, the buzz around ColBERT has left some wondering: What makes ColBERT stand out in the crowded field of information retrieval technologies? Why the AI community is",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "excited about 8192-length ColBERT? This article delves into the intricacies of ColBERT and ColBERTv2, highlighting their design, improvements, and the surprising effectiveness of ColBERT's late interaction. The name \"ColBERT\" stands for Contextualized Late Interaction over BERT, a model stems from",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "the Stanford University, that leverages the deep language understanding of BERT while introducing a novel interaction mechanism. This mechanism, known as late interaction, allows for efficient and precise retrieval by processing queries and documents separately until the final stages of the",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "retrieval process. Specifically, there are two versions of the model: The original ColBERT paper that introduces the \"late interaction\". ColBERTv2 adding denoised supervision and residual compression to improve the training data's quality and reduce the space footprint. Given that ColBERTv2's",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "architecture remains very similar to that of the original ColBERT, with its key innovations revolving around training techniques and compression mechanisms, we will first delve into the foundational aspects of the original ColBERT. \"Interaction\" refers to the process of evaluating the relevance",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "between a query and a document by comparing their representations. \"Late interaction\" is the essence of ColBERT. The term is derived from the model's architecture and processing strategy, where the interaction between the query and document representations occurs late in the process, after both have",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "been independently encoded. This contrasts with \"early interaction\" models, where query and document embeddings interact at earlier stages, typically before or during their encoding by the model.  Early interaction can increase computational complexity since it requires considering all possible",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "query-document pairs, making it less efficient for large-scale applications.  Late interaction models like ColBERT optimize for efficiency and scalability by allowing for the pre-computation of document representations and employing a more lightweight interaction step at the end, which focuses on",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "the already encoded representations. This design choice enables faster retrieval times and reduced computational demands, making it more suitable for processing large document collections. Many practical vector databases and neural search solutions depend on fast cosine similarity matching between",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "document and query embeddings. While appealing for its straightforwardness and computational efficiency, this method, often referred to as \"no interaction\" or \"not interaction-based\" has been found to underperform in comparison to models that incorporate some form of interaction between queries and",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "documents. The core limitation of the \"no interaction\" approach lies in its inability to capture the complex nuances and relationships between query and document terms. Information retrieval, at its heart, is about understanding and matching the intent behind a query with the content within a",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "document. This process often requires a deep, contextual understanding of the terms involved, something that single, aggregated embeddings for documents and queries struggle to provide. ColBERT's encoding strategy is grounded in the BERT model, known for its deep contextual understanding of",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "language. The model generates dense vector representations for each token in a query or document, creating a bag of contextualized embeddings for a query and a bag for a document, respectively. This facilitates a nuanced comparison of their embeddings during the late interaction phase. For a query",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "QQQ with tokens q1,q2,...,ql{q_1, q_2, ..., q_l}q1​,q2​,...,ql​, the process begins by tokenizing QQQ into BERT-based WordPiece tokens and prepending a special [Q] token. This [Q] token, positioned right after BERT’s [CLS] token, signals the start of a query.  If the query is shorter than a",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "predefined number of tokens NqN_qNq​, it is padded with [mask] tokens up to NqN_qNq​; otherwise, it's truncated to the first NqN_qNq​ tokens. The padded sequence is then passed through BERT, followed by a CNN (Convolutional Neural Network) and normalization. The output is a set of embedding vectors",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "termed as Eq\\mathbf{E}_qEq​ below:Eq:=Normalize(BERT([Q],q0,q1,…,ql[mask],[mask],…,[mask]))\\mathbf{E}_q :=",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)Eq​:=Normalize(BERT([Q],q0​,q1​,…,ql​[mask],[mask],…,[mask])) Similarly, for a document DDD with tokens d1,d2,...,dn{d_1, d_2, ..., d_n}d1​,d2​,...,dn​, a [D]",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "token is prepended to indicate the start of a document. This sequence, without the need for padding, undergoes the same process, results in a set of embedding vectors termed as Ed\\mathbf{E}_dEd​ below:Ed:=Filter(Normalize(BERT([D],d0,d1,...,dn)))\\mathbf{E}_d :=",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "\\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)Ed​:=Filter(Normalize(BERT([D],d0​,d1​,...,dn​))) The use of [mask] tokens for padding queries (coined as query augmentation in the paper) ensures uniform length across all queries,",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "facilitating batch processing. The [Q] and [D] tokens explicitly mark the start of queries and documents, respectively, aiding the model in distinguishing between the two types of inputs. Cross-encoders process pairs of queries and documents together, making them highly accurate but less efficient",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "for large-scale tasks due to the computational cost of evaluating every possible pair. They excel in specific scenarios where the precise scoring of sentence pairs is necessary, such as in semantic similarity tasks or detailed content comparison. However, this design limits their applicability in",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "situations requiring rapid retrieval from large datasets, where pre-computed embeddings and efficient similarity calculations are paramount. In contrast, ColBERT’s late interaction model allows for pre-computation of document embeddings, significantly speeding up the retrieval process without",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "compromising the depth of semantic analysis. This method, though seemingly counter-intuitive when compared to the direct approach of cross-encoders, offers a scalable solution for real-time and large-scale information retrieval tasks. It represents a strategic compromise between computational",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "efficiency and the quality of interaction modeling. Once we have embeddings for the query and documents, finding the most relevant top-K documents becomes straightforward (but not as straightforward as computing cosine of two vectors).  The key operations include a batch dot-product to compute term-",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "wise similarities, max-pooling across document terms to find the highest similarity per query term, and summation across query terms to derive the total document score, followed by sorting the documents based on these scores. The pseudo PyTorch code is described below: Note that this procedure is",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "used in both training and re-ranking at inference time. The ColBERT model is trained using a pairwise ranking loss, where the training data consists of triples (q,d+,d−)(q, d^+, d^-)(q,d+,d−), where qqq represents a query, d+d^+d+ is a relevant (positive) document for the query, and d−d^-d− is a",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "non-relevant (negative) document. The model aims to learn representations such that the similarity score between qqq and d+d^+d+ is higher than the score between q and d−d^-d−. The training objective can be mathematically represented as minimizing the following loss function:",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "Loss=max⁡(0,1−S(q,d+)+S(q,d−))\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))Loss=max(0,1−S(q,d+)+S(q,d−)) , where S(q,d)S(q, d)S(q,d) denotes the similarity score computed by ColBERT between a query qqq and a document ddd. This score is obtained by aggregating the max-similarity scores of the",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "best-matching embeddings between the query and the document, following the late interaction pattern described in the model architecture. This approach ensures that the model is trained to distinguish between relevant and irrelevant documents for a given query, by encouraging a larger margin in the",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "similarity scores for positive and negative document pairs. Denoised supervision in ColBERTv2 refines the original training process by selecting challenging negatives and leveraging a cross-encoder for distillation. This sophisticated method of augmenting training data quality involves several",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "steps: Note, this process represents a sophisticated enhancement to the ColBERT training regime rather than a fundamental change to its architecture. The hyperparameters of ColBERT is summarized below: Unlike representation-based approaches that encode each document into one embedding vector,",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "ColBERT encodes documents (and queries) into bags of embeddings, with each token in a document having its own embedding. This approach inherently means that for longer documents, more embeddings will be stored, which is a pain point of  the original ColBERT, and later addressed by ColBERTv2. The key",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "to managing this efficiently lies in ColBERT's use of vector database (e.g. FAISS) for indexing and retrieval, and its detailed indexing process which is designed to handle large volumes of data efficiently. The original ColBERT paper mentions several strategies to enhance the efficiency of indexing",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "and retrieval, including: The introduction of residual compression in ColBERTv2, which is a novel approach not present in the original ColBERT, plays a key role in reducing the model's space footprint by 6–10× while preserving quality. This technique compresses the embeddings further by effectively",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "capturing and storing only the differences from a set of fixed reference centroids.  One might initially assume that incorporating BERT's deep contextual understanding into search would inherently require significant computational resources, making such an approach less feasible for real-time",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "applications due to high latency and computational costs. However, ColBERT challenges and overturns this assumption through its innovative use of the late interaction mechanism. Here are some noteworthy points: Jina-ColBERT is designed for both fast and accurate retrieval, supporting longer context",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "lengths up to 8192, leveraging the advancements of JinaBERT, which allows for longer sequence processing due to its architecture enhancements. Jina-ColBERT's main advancement is its backbone, jina-bert-v2-base-en, which enables processing of significantly longer contexts (up to 8192 tokens) compared",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "to the original ColBERT that uses bert-base-uncased. This capability is crucial for handling documents with extensive content, providing more detailed and contextual search results. We evaluated jina-colbert-v1-en on BEIR datasets and new LoCo benchmark which favors long-context, tested it against",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "the original ColBERTv2 implementation and non-interaction based jina-embeddings-v2-base-en model. This table demonstrates jina-colbert-v1-en's superior  performance, especially in scenarios requiring longer context lengths vs the original ColBERTv2. Note that jina-embeddings-v2-base-en uses more",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "training data, whereas jina-colbert-v1-en only uses MSMARCO, which may justify the good performance of jina-embeddings-v2-base-en on some tasks. This snippet outlines the indexing process with Jina-ColBERT, showcasing its support for long documents. RAGatouille is a new Python library that",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "facilitates the use of advanced retrieval methods within RAG pipelines. It's designed for modularity and easy integration, allowing users to leverage state-of-the-art research seamlessly. The main goal of RAGatouille is to simplify the application of complex models like ColBERT in RAG pipelines,",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "making it accessible for developers to utilize these methods without needing deep expertise in the underlying research. Thanks to Benjamin Clavié, you can now use jina-colbert-v1-en easily: For more detailed information and further exploration of Jina-ColBERT, you can visit the Hugging Face page.",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "ColBERT represents a significant leap forward in the field of information retrieval. By enabling longer context lengths with Jina-ColBERT and maintaining compatibility with the ColBERT approach to late interaction, it offers a powerful alternative for developers looking to implement state-of-the-art",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "search functionality. Coupled with the RAGatouille library, which simplifies the integration of complex retrieval models into RAG pipelines, developers can now harness the power of advanced retrieval with ease, streamlining their workflows and enhancing their applications. The synergy between Jina-",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "ColBERT and RAGatouille illustrates a remarkable stride in making advanced AI search models accessible and efficient for practical use.",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "Something went wrong. Wait a moment and try again.",
    "url": "https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora"
  },
  {
    "text": "Jul 25, 2024                       • Chip Huyen  After studying how companies deploy generative AI applications, I noticed many similarities in their platforms. This post outlines the common components of a generative AI platform, what they do, and how they are implemented. I try my best to keep the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "architecture general, but certain applications might deviate. This is what the overall architecture looks like.  This is a pretty complex system. This post will start from the simplest architecture and progressively add more components. In its simplest form, your application receives a query and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "sends it to the model. The model generates a response, which is returned to the user. There are no guardrails, no augmented context, and no optimization. The Model API box refers to both third-party APIs (e.g., OpenAI, Google, Anthropic) and self-hosted APIs.  From this, you can add more components",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "as needs arise. The order discussed in this post is common, though you don’t need to follow the exact same order. A component can be skipped if your system works well without it. Evaluation is necessary at every step of the development process. Observability, which allows you to gain visibility into",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "your system for monitoring and debugging, and orchestration, which involves chaining all the components together, are two essential components of the platform. We will discuss them at the end of this post. » What this post is not « This post focuses on the overall architecture for deploying AI",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "applications. It discusses what components are needed and considerations when building these components. It’s not about how to build AI applications and, therefore, does NOT discuss model evaluation, application evaluation, prompt engineering, finetuning, data annotation guidelines, or chunking",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "strategies for RAGs. All these topics are covered in my upcoming book AI Engineering.  The initial expansion of a platform usually involves adding mechanisms to allow the system to augment each query with the necessary information. Gathering the relevant information is called context construction.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Many queries require context to answer. The more relevant information there is in the context, the less the model has to rely on its internal knowledge, which can be unreliable due to its training data and training methodology. Studies have shown that having access to relevant information in the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "context can help the model generate more detailed responses while reducing hallucinations (Lewis et al., 2020). For example, given the query “Will Acme’s fancy-printer-A300 print 100pps?”, the model will be able to respond better if it’s given the specifications of fancy-printer-A300. (Thanks Chetan",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Tekur for the example.) Context construction for foundation models is equivalent to feature engineering for classical ML models. They serve the same purpose: giving the model the necessary information to process an input. In-context learning, learning from the context, is a form of continual",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "learning. It enables a model to incorporate new information continually to make decisions, preventing it from becoming outdated. For example, a model trained on last-week data won’t be able to answer questions about this week unless the new information is included in its context. By updating a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model’s context with the latest information, e.g. fancy-printer-A300’s latest specifications, the model remains up-to-date and can respond to queries beyond its cut-off date. The most well-known pattern for context construction is RAG, Retrieval-Augmented Generation. RAG consists of two components:",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "a generator (e.g. a language model) and a retriever, which retrieves relevant information from external sources.  Retrieval isn’t unique to RAGs. It’s the backbone of search engines, recommender systems, log analytics, etc. Many retrieval algorithms developed for traditional retrieval systems can be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "used for RAGs. External memory sources typically contain unstructured data, such as memos, contracts, news updates, etc. They can be collectively called documents. A document can be 10 tokens or 1 million tokens. Naively retrieving whole documents can cause your context to be arbitrarily long. RAG",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "typically requires documents to be split into manageable chunks, which can be determined from the model’s maximum context length and your application’s latency requirements. To learn more about chunking and the optimal chunk size, see Pinecone, Langchain, Llamaindex, and Greg Kamradt’s tutorials.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Once data from external memory sources has been loaded and chunked, retrieval is performed using two main approaches. Embedding-based retrieval (also known as vector search)  You convert chunks of data into embedding vectors using an embedding model such as BERT, sentence-transformers, and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "proprietary embedding models provided by OpenAI or Google. Given a query, the data whose vectors are closest to the query embedding, as determined by the vector search algorithm, is retrieved.   Vector search is usually framed as nearest-neighbor search, using approximate nearest neighbor (ANN)",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "algorithms such as FAISS (Facebook AI Similarity Search), Google’s ScaNN, Spotify’s ANNOY, and hnswlib (Hierarchical Navigable Small World).    The ANN-benchmarks website compares different ANN algorithms on multiple datasets using four main metrics, taking into account the tradeoffs between",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "indexing and querying. This works with not just text documents, but also images, videos, audio, and code. Many teams even try to summarize SQL tables and dataframes and then use these summaries to generate embeddings for retrieval. Term-based retrieval is much faster and cheaper than embedding-based",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "retrieval. It can work well out of the box, making it an attractive option to start. Both BM25 and Elasticsearch are widely used in the industry and serve as formidable baselines for more complex retrieval systems. Embedding-based retrieval, while computationally expensive, can be significantly",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "improved over time to outperform term-based retrieval. A production retrieval system typically combines several approaches. Combining term-based retrieval and embedding-based retrieval is called hybrid search. One common pattern is sequential. First, a cheap, less precise retriever, such as a term-",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "based system, fetches candidates. Then, a more precise but more expensive mechanism, such as k-nearest neighbors, finds the best of these candidates. The second step is also called reranking. For example, given the term “transformer”, you can fetch all documents that contain the word transformer,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "regardless of whether they are about the electric device, the neural architecture, or the movie. Then you use vector search to find among these documents those that are actually related to your transformer query. Context reranking differs from traditional search reranking in that the exact position",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "of items is less critical. In search, the rank (e.g., first or fifth) is crucial. In context reranking, the order of documents still matters because it affects how well a model can process them. Models might better understand documents at the beginning and end of the context, as suggested by the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "paper Lost in the middle (Liu et al., 2023). However, as long as a document is included, the impact of its order is less significant compared to in search ranking. Another pattern is ensemble. Remember that a retriever works by ranking documents by their relevance scores to the query. You use",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "multiple retrievers to fetch candidates at the same time, then combine these different rankings together to generate a final ranking. External data sources can also be structured, such as dataframes or SQL tables. Retrieving data from an SQL table is significantly different from retrieving data from",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "unstructured documents. Given a query, the system works as follows.  For the text-to-SQL step, if there are many available tables whose schemas can’t all fit into the model context, you might need an intermediate step to predict what tables to use for each query. Text-to-SQL can be done by the same",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model used to generate the final response or one of many specialized text-to-SQL models. An important source of data is the Internet. A web search tool like Google or Bing API can give the model access to a rich, up-to-date resource to gather relevant information for each query. For example, given",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "the query “Who won Oscar this year?”, the system searches for information about the latest Oscar and uses this information to generate the final response to the user. Term-based retrieval, embedding-based retrieval, SQL execution, and web search are actions that a model can take to augment its",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "context. You can think of each action as a function the model can call. A workflow that can incorporate external actions is also called agentic. The architecture then looks like this.  » Action vs. tool « A tool allows one or more actions. For example, a people search tool might allow two actions:",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "search by name and search by email. However, the difference is minimal, so many people use action and tool interchangeably. » Read-only actions vs. write actions « Actions that retrieve information from external sources but don’t change their states are read-only actions. Giving a model write",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "actions, e.g. updating the values in a table, enables the model to perform more tasks but also poses more risks, which will be discussed later. Often, a user query needs to be rewritten to increase the likelihood of fetching the right information. Consider the following conversation. The last",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "question, “How about Emily Doe?”, is ambiguous. If you use this query verbatim to retrieve documents, you’ll likely get irrelevant results. You need to rewrite this query to reflect what the user is actually asking. The new query should make sense on its own. The last question should be rewritten to",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "“When was the last time Emily Doe bought something from us?” Query rewriting is typically done using other AI models, using a prompt similar to “Given the following conversation, rewrite the last user input to reflect what the user is actually asking.”  Query rewriting can get complicated,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "especially if you need to do identity resolution or incorporate other knowledge. If the user asks “How about his wife?”, you will first need to query your database to find out who his wife is. If you don’t have this information, the rewriting model should acknowledge that this query isn’t solvable",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "instead of hallucinating a name, leading to a wrong answer. Guardrails help reduce AI risks and protect not just your users but also you, the developers. They should be placed whenever there is potential for failures. This post discusses two types of guardrails: input guardrails and output",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "guardrails. Input guardrails are typically protection against two types of risks: leaking private information to external APIs, and executing bad prompts that compromise your system (model jailbreaking). This risk is specific to using external model APIs when you need to send your data outside your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "organization. For example, an employee might copy the company’s secret or a user’s private information into a prompt and send it to wherever the model is hosted.   One of the most notable early incidents was when Samsung employees put Samsung’s proprietary information into ChatGPT, accidentally",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "leaking the company’s secrets. It’s unclear how Samsung discovered this leak and how the leaked information was used against Samsung. However, the incident was serious enough for Samsung to ban ChatGPT in May 2023.   There’s no airtight way to eliminate potential leaks when using third-party APIs.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "However, you can mitigate them with guardrails. You can use one of the many available tools that automatically detect sensitive data. What sensitive data to detect is specified by you. Common sensitive data classes are: Many sensitive data detection tools use AI to identify potentially sensitive",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "information, such as determining if a string resembles a valid home address. If a query is found to contain sensitive information, you have two options: block the entire query or remove the sensitive information from it. For instance, you can mask a user’s phone number with the placeholder [PHONE",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "NUMBER]. If the generated response contains this placeholder, use a PII reversible dictionary that maps this placeholder to the original information so that you can unmask it, as shown below.  It’s become an online sport to try to jailbreak AI models, getting them to say or do bad things. While some",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "might find it amusing to get ChatGPT to make controversial statements, it’s much less fun if your customer support chatbot, branded with your name and logo, does the same thing. This can be especially dangerous for AI systems that have access to tools. Imagine if a user finds a way to get your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "system to execute an SQL query that corrupts your data.   To combat this, you should first put guardrails on your system so that no harmful actions can be automatically executed. For example, no SQL queries that can insert, delete, or update data can be executed without human approval. The downside",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "of this added security is that it can slow down your system. To prevent your application from making outrageous statements it shouldn’t be making, you can define out-of-scope topics for your application. For example, if your application is a customer support chatbot, it shouldn’t answer political or",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "social questions. A simple way to do so is to filter out inputs that contain predefined phrases typically associated with controversial topics, such as “immigration” or “antivax”. More sophisticated algorithms use AI to classify whether an input is about one of the pre-defined restricted topics.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "If harmful prompts are rare in your system, you can use an anomaly detection algorithm to identify unusual prompts. AI models are probabilistic, making their outputs unreliable. You can put in guardrails to significantly improve your application’s reliability. Output guardrails have two main",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "functionalities: To catch outputs that fail to meet your standards, you need to understand what failures look like. Here are examples of failure modes and how to catch them. Empty responses. Malformatted responses that don’t follow the expected output format. For example, if the application expects",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "JSON and the generated response has a missing closing bracket. There are validators for certain formats, such as regex, JSON, and Python code validators. There are also tools for constrained sampling such as guidance, outlines, and instructor. Toxic responses, such as those that are racist or",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "sexist. These responses can be caught using one of many toxicity detection tools. Factual inconsistent responses hallucinated by the model. Hallucination detection is an active area of research with solutions such as SelfCheckGPT (Manakul et al., 2023) and SAFE, Search Engine Factuality Evaluator",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "(Wei et al., 2024). You can mitigate hallucinations by providing models with sufficient context and prompting techniques such as chain-of-thought. Hallucination detection and mitigation are discussed further in my upcoming book AI Engineering. This failure mode can be prevented by not training your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model on sensitive data and not allowing it to retrieve sensitive data in the first place. Sensitive data in outputs can be detected using the same tools used for input guardrails. Brand-risk responses, such as responses that mischaracterize your company or your competitors. An example is when Grok,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "a model trained by X, generated a response suggesting that Grok was trained by OpenAI, causing the Internet to suspect X of stealing OpenAI’s data. This failure mode can be mitigated with keyword monitoring. Once you’ve identified outputs concerning your brands and competitors, you can either block",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "these outputs, pass them onto human reviewers, or use other models to detect the sentiment of these outputs to ensure that only the right sentiments are returned. AI models are probabilistic, which means that if you try a query again, you might get a different response. Many failures can be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "mitigated using a basic retry logic. For example, if the response is empty, try again X times or until you get a non-empty response. Similarly, if the response is malformatted, try again until the model generates a correctly formatted response. This retry policy, however, can incur extra latency and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "cost. One retry means 2x the number of API calls. If the retry is carried out after failure, the latency experienced by the user will double. To reduce latency, you can make calls in parallel. For example, for each query, instead of waiting for the first query to fail before retrying, you send this",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "query to the model twice at the same time, get back two responses, and pick the better one. This increases the number of redundant API calls but keeps latency manageable. It’s also common to fall back on humans to handle tricky queries. For example, you can transfer a query to human operators if it",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "contains specific key phrases. Some teams use a specialized model, potentially trained in-house, to decide when to transfer a conversation to humans. One team, for instance, transfers a conversation to human operators when their sentiment analysis model detects that the user is getting angry.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Another team transfers a conversation after a certain number of turns to prevent users from getting stuck in an infinite loop. Reliability vs. latency tradeoff: While acknowledging the importance of guardrails, some teams told me that latency is more important. They decided not to implement",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "guardrails because they can significantly increase their application’s latency. However, these teams are in the minority. Most teams find that the increased risks are more costly than the added latency. Output guardrails might not work well in the stream completion mode. By default, the whole",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "response is generated before shown to the user, which can take a long time. In the stream completion mode, new tokens are streamed to the user as they are generated, reducing the time the user has to wait to see the response. The downside is that it’s hard to evaluate partial responses, so unsafe",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "responses might be streamed to users before the system guardrails can determine that they should be blocked. Self-hosted vs. third-party API tradeoff: Self-hosting your models means that you don’t have to send your data to a third party, reducing the need for input guardrails. However, it also means",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "that you must implement all the necessary guardrails yourself, rather than relying on the guardrails provided by third-party services. Our platform now looks like this. Guardrails can be independent tools or parts of model gateways, as discussed later. Scorers, if used, are grouped under model APIs",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "since scorers are typically AI models, too. Models used for scoring are typically smaller and faster than models used for generation.  As applications grow in complexity and involve more models, two types of tools emerged to help you work with multiple models: routers and gateways. An application",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "can use different models to respond to different types of queries. Having different solutions for different queries has several benefits. First, this allows you to have specialized solutions, such as one model specialized in technical troubleshooting and another specialized in subscriptions.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Specialized models can potentially perform better than a general-purpose model. Second, this can help you save costs. Instead of routing all queries to an expensive model, you can route simpler queries to cheaper models. A router typically consists of an intent classifier that predicts what the user",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "is trying to do. Based on the predicted intent, the query is routed to the appropriate solution. For example, for a customer support chatbot, if the intent is: An intent classifier can also help your system avoid out-of-scope conversations. For example, you can have an intent classifier that",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "predicts whether a query is out of the scope. If the query is deemed inappropriate (e.g. if the user asks who you would vote for in the upcoming election), the chatbot can politely decline to engage using one of the stock responses (“As a chatbot, I don’t have the ability to vote. If you have",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "questions about our products, I’d be happy to help.”) without wasting an API call. If your system has access to multiple actions, a router can involve a next-action predictor to help the system decide what action to take next. One valid action is to ask for clarification if the query is ambiguous.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "For example, in response to the query “Freezing,” the system might ask, “Do you want to freeze your account or are you talking about the weather?” or simply say, “I’m sorry. Can you elaborate?” Intent classifiers and next-action predictors can be general-purpose models or specialized classification",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "models. Specialized classification models are typically much smaller and faster than general-purpose models, allowing your system to use multiple of them without incurring significant extra latency and cost. When routing queries to models with varying context limits, the query’s context might need",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to be adjusted accordingly. Consider a query of 1,000 tokens that is slated for a model with a 4K context limit. The system then takes an action, e.g. web search, that brings back 8,000-token context. You can either truncate the query’s context to fit the originally intended model or route the query",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to a model with a larger context limit. A model gateway is an intermediate layer that allows your organization to interface with different models in a unified and secure manner. The most basic functionality of a model gateway is to enable developers to access different models – be it self-hosted",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "models or models behind commercial APIs such as OpenAI or Google – the same way. A model gateway makes it easier to maintain your code. If a model API changes, you only need to update the model gateway instead of having to update all applications that use this model API.  In its simplest form, a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model gateway is a unified wrapper that looks like the following code example. This example is to give you an idea of how a model gateway might be implemented. It’s not meant to be functional as it doesn’t contain any error checking or optimization. A model gateway is access control and cost",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "management. Instead of giving everyone who wants access to the OpenAI API your organizational tokens, which can be easily leaked, you only give people access to the model gateway, creating a centralized and controlled point of access. The gateway can also implement fine-grained access controls,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "specifying which user or application should have access to which model. Moreover, the gateway can monitor and limit the usage of API calls, preventing abuse and managing costs effectively. A model gateway can also be used to implement fallback policies to overcome rate limits or API failures (the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "latter is unfortunately common). When the primary API is unavailable, the gateway can route requests to alternative models, retry after a short wait, or handle failures in other graceful manners. This ensures that your application can operate smoothly without interruptions. Since requests and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "responses are already flowing through the gateway, it’s a good place to implement other functionalities such as load balancing, logging, and analytics. Some gateway services even provide caching and guardrails. Given that gateways are relatively straightforward to implement, there are many off-the-",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "shelf gateways. Examples include Portkey’s gateway, MLflow AI Gateway, WealthSimple’s llm-gateway, TrueFoundry, Kong, and Cloudflare. With the added gateway and routers, our platform is getting more exciting. Like scoring, routing is also in the model gateway. Like models used for scoring, models",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "used for routing are typically smaller than models used for generation.  When I shared this post with my friend Eugene Yan, he said that cache is perhaps the most underrated component of an AI platform. Caching can significantly reduce your application’s latency and cost. Cache techniques can also",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "be used during training, but since this post is about deployment, I’ll focus on cache for inference. Some common inference caching techniques include prompt cache, exact cache, and semantic cache. Prompt cache are typically implemented by the inference APIs that you use. When evaluating an inference",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "library, it’s helpful to understand what cache mechanism it supports. KV cache for the attention mechanism is out of scope for this discussion. Many prompts in an application have overlapping text segments. For example, all queries can share the same system prompt. A prompt cache stores these",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "overlapping segments for reuse, so you only need to process them once. A common overlapping text segment in different prompts is the system prompt. Without prompt cache, your model needs to process the system prompt with every query. With prompt cache, it only needs to process the system prompt once",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "for the first query. For applications with long system prompts, prompt cache can significantly reduce both latency and cost. If your system prompt is 1000 tokens and your application generates 1 million model API calls today, a prompt cache will save you from processing approximately 1 billion",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "repetitive input tokens a day! However, this isn’t entirely free. Like KV cache, prompt cache size can be quite large and require significant engineering effort. Prompt cache is also useful for queries that involve long documents. For example, if many of your user queries are related to the same",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "long document (such as a book or a codebase), this long document can be cached for reuse across queries. Since its introduction in November 2023 by Gim et al., prompt cache has already been incorporated into model APIs. Google announced that Gemini APIs will offer this functionality in June 2024",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "under the name context cache. Cached input tokens are given a 75% discount compared to regular input tokens, but you’ll have to pay extra for cache storage (as of writing, $1.00 / 1 million tokens per hour). Given the obvious benefits of prompt cache, I wouldn’t be surprised if it becomes as popular",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "as KV cache. While llama.cpp also has prompt cache, it seems to only cache whole prompts and work for queries in the same chat session. Its documentation is limited, but my guess from reading the code is that in a long conversation, it caches the previous messages and only processes the newest",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "message. If prompt cache and KV cache are unique to foundation models, exact cache is more general and straightforward. Your system stores processed items for reuse later when the exact items are requested. For example, if a user asks a model to summarize a product, the system checks the cache to",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "see if a summary of this product is cached. If yes, fetch this summary. If not, summarize the product and cache the summary. Exact cache is also used for embedding-based retrieval to avoid redundant vector search. If an incoming query is already in the vector search cache, fetch the cached search",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "result. If not, perform a vector search for this query and cache the result. Cache is especially appealing for queries that require multiple steps (e.g. chain-of-thought) and/or time-consuming actions (e.g. retrieval, SQL execution, or web search). An exact cache can be implemented using in-memory",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "storage for fast retrieval. However, since in-memory storage is limited, a cache can also be implemented using databases like PostgreSQL, Redis, or tiered storage to balance speed and storage capacity. Having an eviction policy is crucial to manage the cache size and maintain performance. Common",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "eviction policies include Least Recently Used (LRU), Least Frequently Used (LFU), and First In, First Out (FIFO). How long to cache a query depends on how likely this query is to be called again. User-specific queries such as “What’s the status of my recent order” are less likely to be reused by",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "other users, and therefore, shouldn’t be cached. Similarly, it makes less sense to cache time-sensitive queries such as “How’s the weather?” Some teams train a small classifier to predict whether a query should be cached. Unlike exact cache, semantic cache doesn’t require the incoming query to be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "identical to any of the cached queries. Semantic cache allows the reuse of similar queries. Imagine one user asks “What’s the capital of Vietnam?” and the model generates the answer “Hanoi”. Later, another user asks “What’s the capital city of Vietnam?”, which is the same question but with the extra",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "word “city”. The idea of semantic cache is that the system can reuse the answer “Hanoi” instead of computing the new query from scratch. Semantic cache only works if you have a reliable way to determine if two queries are semantically similar. One common approach is embedding-based similarity, which",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "works as follows: This approach requires a vector database to store the embeddings of cached queries. Compared to other caching techniques, semantic cache’s value is more dubious because many of its components are prone to failure. Its success relies on high-quality embeddings, functional vector",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "search, and a trustworthy similarity metric. Setting the right similarity threshold can also be tricky and require a lot of trial and error. If the system mistakes the incoming query as being similar to another query, the returned response, fetched from the cache, will be incorrect. In addition,",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "semantic cache can be time-consuming and compute-intensive, as it involves a vector search. The speed and cost of this vector search depend on the size of your database of cached embeddings. Semantic cache might still be worth it if the cache hit rate is high, meaning that a good portion of queries",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "can be effectively answered by leveraging the cached results. However, before incorporating the complexities of semantic cache, make sure to evaluate the efficiency, cost, and performance risks associated with it. With the added cache systems, the platform looks as follows. KV cache and prompt cache",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "are typically implemented by model API providers, so they aren’t shown in this image. If I must visualize them, I’d put them in the Model API box. There’s a new arrow to add generated responses to the cache.  The applications we’ve discussed so far have fairly simple flows. The outputs generated by",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "foundation models are mostly returned to users (unless they don’t pass the guardrails). However, an application flow can be more complex with loops and conditional branching. A model’s outputs can also be used to invoke write actions, such as composing an email or placing an order. Outputs from a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "model can be conditionally passed onto another model or fed back to the same model as part of the input to the next step. This goes on until a model in the system decides that the task has been completed and that a final response should be returned to the user. This can happen when you give your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "system the ability to plan and decide what to do next. As an example, consider the query “Plan a weekend itinerary for Paris.” The model might first generate a list of potential activities: visiting the Eiffel Tower, having lunch at a café, touring the Louvre, etc. Each of these activities can then",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "be fed back into the model to generate more detailed plans. For instance, “visiting the Eiffel Tower” could prompt the model to generate sub-tasks like checking the opening hours, buying tickets, and finding nearby restaurants. This iterative process continues until a comprehensive and detailed",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "itinerary is created. Our infrastructure now has an arrow pointing the generated response back to context construction, which in turn feeds back to models in the model gateway.  Actions used for context construction are read-only actions. They allow a model to read from its data sources to gather",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "context. But a system can also write actions, making changes to the data sources and the world. For example, if the model outputs: “send an email to X with the message Y”, the system will invoke the action send_email(recipient=X, message=Y). Write actions make a system vastly more capable. They can",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "enable you to automate the whole customer outreach workflow: researching potential customers, finding their contacts, drafting emails, sending first emails, reading responses, following up, extracting orders, updating your databases with new orders, etc. However, the prospect of giving AI the",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "ability to automatically alter our lives is frightening. Just as you shouldn’t give an intern the authority to delete your production database, you shouldn’t allow an unreliable AI to initiate bank transfers. Trust in the system’s capabilities and its security measures is crucial. You need to ensure",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "that the system is protected from bad actors who might try to manipulate it into performing harmful actions. AI systems are vulnerable to cyber attacks like other software systems, but they also have another weakness: prompt injection. Prompt injection happens when an attacker manipulates input",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "prompts into a model to get it to express undesirable behaviors. You can think of prompt injection as social engineering done on AI instead of humans. A scenario that many companies fear is that they give an AI system access to their internal databases, and attackers trick this system into revealing",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "private information from these databases. If the system has write access to these databases, attackers can trick the system into corrupting the data. Any organization that wants to leverage AI needs to take safety and security seriously. However, these risks don’t mean that AI systems should never",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "be given the ability to act in the real world. AI systems can fail, but humans can fail too. If we can get people to trust a machine to take us up into space, I hope that one day, securities will be sufficient for us to trust autonomous AI systems.  While I have placed observability in its own",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "section, it should be integrated into the platform from the beginning rather than added later as an afterthought. Observability is crucial for projects of all sizes, and its importance grows with the complexity of the system. This section provides the least information compared to the others. It’s",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "impossible to cover all the nuances of observability in a blog post. Therefore, I will only give a brief overview of the three pillars of monitoring: logs, traces, and metrics. I won’t go into specifics or cover user feedback, drift detection, and debugging. When discussing monitoring, most people",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "think of metrics. What metrics to track depends on what you want to track about your system, which is application-specific. However, in general, there are two types of metrics you want to track: model metrics and system metrics. System metrics tell you the state of your overall system. Common",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "metrics are throughput, memory usage, hardware utilization, and service availability/uptime. System metrics are common to all software engineering applications. In this post, I’ll focus on model metrics. Model metrics assess your model’s performance, such as accuracy, toxicity, and hallucination",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "rate. Different steps in an application pipeline also have their own metrics. For example, in a RAG application, the retrieval quality is often evaluated using context relevance and context precision. A vector database can be evaluated by how much storage it needs to index the data and how long it",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "takes to query the data There are various ways a model’s output can fail. It’s crucial to identify these issues and develop metrics to monitor them. For example, you might want to track how often your model times out, returns empty responses or produces malformatted responses. If you’re worried",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "about your model revealing sensitive information, find a way to track that too. Length-related metrics such as query, context, and response length are helpful for understanding your model’s behaviors. Is one model more verbose than another? Are certain types of queries more likely to result in",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "lengthy answers? They are especially useful for detecting changes in your application. If the average query length suddenly decreases, it could indicate an underlying issue that needs investigation. Length-related metrics are also important for tracking latency and costs, as longer contexts and",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "responses typically increase latency and incur higher costs. Tracking latency is essential for understanding the user experience. Common latency metrics include: You’ll also want to track costs. Cost-related metrics are the number of queries and the volume of input and output tokens. If you use an",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "API with rate limits, tracking the number of requests per second is important to ensure you stay within your allocated limits and avoid potential service interruptions. When calculating metrics, you can choose between spot checks and exhaustive checks. Spot checks involve sampling a subset of data",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to quickly identify issues, while exhaustive checks evaluate every request for a comprehensive performance view. The choice depends on your system’s requirements and available resources, with a combination of both providing a balanced monitoring strategy. When computing metrics, ensure they can be",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "broken down by relevant axes, such as users, releases, prompt/chain versions, prompt/chain types, and time. This granularity helps in understanding performance variations and identifying specific issues. Since this blog post is getting long and I’ve written at length about logs in Designing Machine",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Learning Systems, I will be quick here. The philosophy for logging is simple: log everything. Log the system configurations. Log the query, the output, and the intermediate outputs. Log when a component starts, ends, when something crashes, etc. When recording a piece of log, make sure to give it",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "tags and IDs that can help you know where in the system this log comes from. Logging everything means that the amount of logs you have can grow very quickly. Many tools for automated log analysis and log anomaly detection are powered by AI. While it’s impossible to manually process logs, it’s useful",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "to manually inspect your production data daily to get a sense of how users are using your application. Shankar et al. (2024) found that the developers’ perceptions of what constitutes good and bad outputs change as they interact with more data, allowing them to both rewrite their prompts to increase",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "the chance of good responses and update their evaluation pipeline to catch bad responses. Trace refers to the detailed recording of a request’s execution path through various system components and services. In an AI application, tracing reveals the entire process from when a user sends a query to",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "when the final response is returned, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. It should also show how much time each step takes and its associated cost, if measurable. As an example, this is a visualization of a Langsmith trace.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Ideally, you should be able to trace each query’s transformation through the system step-by-step. If a query fails, you should be able to pinpoint the exact step where it went wrong: whether it was incorrectly processed, the retrieved context was irrelevant, or the model generated a wrong response.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "An AI application can get fairly complex, consisting of multiple models, retrieving data from many databases, and having access to a wide range of tools. An orchestrator helps you specify how these different components are combined (chained) together to create an end-to-end application flow. At a",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "high level, an orchestrator works in two steps: components definition and chaining (also known as pipelining). Components Definition  You need to tell the orchestrator what components your system uses, such as models (including models for generation, routing, and scoring), databases from which your",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "system can retrieve data, and actions that your system can take. Direct integration with model gateways can help simplify model onboarding, and some orchestrator tools want to be gateways. Many orchestrators also support integration with tools for evaluation and monitoring. Chaining (or pipelining)",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "You tell the orchestrator the sequence of steps your system takes from receiving the user query until completing the task. In short, chaining is just function composition. Here’s an example of what a pipeline looks like. The orchestrator is responsible for passing data between steps and can provide",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "toolings that help ensure that the output from the current step is in the format expected by the next step. When designing the pipeline for an application with strict latency requirements, try to do as much in parallel as possible. For example, if you have a routing component (deciding where to send",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "a query to) and a PII removal component, they can do both at the same time. There are many AI orchestration tools, including LangChain, LlamaIndex, Flowise, Langflow, and Haystack. Each tool has its own APIs so I won’t show the actual code here. While it’s tempting to jump straight to an",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "orchestration tool when starting a project, start building your application without one first. Any external tool brings added complexity. An orchestrator can abstract away critical details of how your system works, making it hard to understand and debug your system. As you advance to the later",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "stages of your application development process, you might decide that an orchestrator can make your life easier. Here are three aspects to keep in mind when evaluating orchestrators. This post started with a basic architecture and then gradually added components to address the growing application",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "complexities. Each addition brings its own set of benefits and challenges, requiring careful consideration and implementation. While the separation of components is important to keep your system modular and maintainable, this separation is fluid. There are many overlaps between components. For",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "example, a model gateway can share functionalities with guardrails. Cache can be implemented in different components, such as in vector search and inference services. This post is much longer than I intended it to be, and yet there are many details I haven’t been able to explore further, especially",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "around observability, context construction, complex logic, cache, and guardrails. I’ll dive deeper into all these components in my upcoming book AI Engineering. This post also didn’t discuss how to serve models, assuming that most people will be using models provided by third-party APIs. AI",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Engineering will also have a chapter dedicated to inference and model optimization. Special thanks to Luke Metz, Alex Li, Chetan Tekur, Kittipat “Bot” Kampa, Hien Luu, and Denys Linkov for feedback on the early versions of this post. Their insights greatly improved the content. Any remaining errors",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "are my own. I read many case studies shared by companies on how they adopted generative AI, and here are some of my favorites. I help companies deploy machine learning into production. I write about AI applications, tooling, and best practices.",
    "url": "https://huyenchip.com/2024/07/25/genai-platform.html"
  },
  {
    "text": "Update: On Aug. 31st 2024, we released the 2nd version of Jina-ColBERT, with improved performance, multilingual support over 89 languages and flexible output dimensions. Check the release post for more details. Last Friday, the release of the ColBERT model by Jina AI on Hugging Face sparked",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "significant excitement across the AI community, particularly on Twitter/X. While many are familiar with the groundbreaking BERT model, the buzz around ColBERT has left some wondering: What makes ColBERT stand out in the crowded field of information retrieval technologies? Why the AI community is",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "excited about 8192-length ColBERT? This article delves into the intricacies of ColBERT and ColBERTv2, highlighting their design, improvements, and the surprising effectiveness of ColBERT's late interaction. The name \"ColBERT\" stands for Contextualized Late Interaction over BERT, a model stems from",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "the Stanford University, that leverages the deep language understanding of BERT while introducing a novel interaction mechanism. This mechanism, known as late interaction, allows for efficient and precise retrieval by processing queries and documents separately until the final stages of the",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "retrieval process. Specifically, there are two versions of the model: The original ColBERT paper that introduces the \"late interaction\". ColBERTv2 adding denoised supervision and residual compression to improve the training data's quality and reduce the space footprint. Given that ColBERTv2's",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "architecture remains very similar to that of the original ColBERT, with its key innovations revolving around training techniques and compression mechanisms, we will first delve into the foundational aspects of the original ColBERT. \"Interaction\" refers to the process of evaluating the relevance",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "between a query and a document by comparing their representations. \"Late interaction\" is the essence of ColBERT. The term is derived from the model's architecture and processing strategy, where the interaction between the query and document representations occurs late in the process, after both have",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "been independently encoded. This contrasts with \"early interaction\" models, where query and document embeddings interact at earlier stages, typically before or during their encoding by the model.  Early interaction can increase computational complexity since it requires considering all possible",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "query-document pairs, making it less efficient for large-scale applications.  Late interaction models like ColBERT optimize for efficiency and scalability by allowing for the pre-computation of document representations and employing a more lightweight interaction step at the end, which focuses on",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "the already encoded representations. This design choice enables faster retrieval times and reduced computational demands, making it more suitable for processing large document collections. Many practical vector databases and neural search solutions depend on fast cosine similarity matching between",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "document and query embeddings. While appealing for its straightforwardness and computational efficiency, this method, often referred to as \"no interaction\" or \"not interaction-based\" has been found to underperform in comparison to models that incorporate some form of interaction between queries and",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "documents. The core limitation of the \"no interaction\" approach lies in its inability to capture the complex nuances and relationships between query and document terms. Information retrieval, at its heart, is about understanding and matching the intent behind a query with the content within a",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "document. This process often requires a deep, contextual understanding of the terms involved, something that single, aggregated embeddings for documents and queries struggle to provide. ColBERT's encoding strategy is grounded in the BERT model, known for its deep contextual understanding of",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "language. The model generates dense vector representations for each token in a query or document, creating a bag of contextualized embeddings for a query and a bag for a document, respectively. This facilitates a nuanced comparison of their embeddings during the late interaction phase. For a query",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "QQQ with tokens q1,q2,...,ql{q_1, q_2, ..., q_l}q1​,q2​,...,ql​, the process begins by tokenizing QQQ into BERT-based WordPiece tokens and prepending a special [Q] token. This [Q] token, positioned right after BERT’s [CLS] token, signals the start of a query.  If the query is shorter than a",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "predefined number of tokens NqN_qNq​, it is padded with [mask] tokens up to NqN_qNq​; otherwise, it's truncated to the first NqN_qNq​ tokens. The padded sequence is then passed through BERT, followed by a CNN (Convolutional Neural Network) and normalization. The output is a set of embedding vectors",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "termed as Eq\\mathbf{E}_qEq​ below:Eq:=Normalize(BERT([Q],q0,q1,…,ql[mask],[mask],…,[mask]))\\mathbf{E}_q :=",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)Eq​:=Normalize(BERT([Q],q0​,q1​,…,ql​[mask],[mask],…,[mask])) Similarly, for a document DDD with tokens d1,d2,...,dn{d_1, d_2, ..., d_n}d1​,d2​,...,dn​, a [D]",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "token is prepended to indicate the start of a document. This sequence, without the need for padding, undergoes the same process, results in a set of embedding vectors termed as Ed\\mathbf{E}_dEd​ below:Ed:=Filter(Normalize(BERT([D],d0,d1,...,dn)))\\mathbf{E}_d :=",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "\\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)Ed​:=Filter(Normalize(BERT([D],d0​,d1​,...,dn​))) The use of [mask] tokens for padding queries (coined as query augmentation in the paper) ensures uniform length across all queries,",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "facilitating batch processing. The [Q] and [D] tokens explicitly mark the start of queries and documents, respectively, aiding the model in distinguishing between the two types of inputs. Cross-encoders process pairs of queries and documents together, making them highly accurate but less efficient",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "for large-scale tasks due to the computational cost of evaluating every possible pair. They excel in specific scenarios where the precise scoring of sentence pairs is necessary, such as in semantic similarity tasks or detailed content comparison. However, this design limits their applicability in",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "situations requiring rapid retrieval from large datasets, where pre-computed embeddings and efficient similarity calculations are paramount. In contrast, ColBERT’s late interaction model allows for pre-computation of document embeddings, significantly speeding up the retrieval process without",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "compromising the depth of semantic analysis. This method, though seemingly counter-intuitive when compared to the direct approach of cross-encoders, offers a scalable solution for real-time and large-scale information retrieval tasks. It represents a strategic compromise between computational",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "efficiency and the quality of interaction modeling. Once we have embeddings for the query and documents, finding the most relevant top-K documents becomes straightforward (but not as straightforward as computing cosine of two vectors).  The key operations include a batch dot-product to compute term-",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "wise similarities, max-pooling across document terms to find the highest similarity per query term, and summation across query terms to derive the total document score, followed by sorting the documents based on these scores. The pseudo PyTorch code is described below: Note that this procedure is",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "used in both training and re-ranking at inference time. The ColBERT model is trained using a pairwise ranking loss, where the training data consists of triples (q,d+,d−)(q, d^+, d^-)(q,d+,d−), where qqq represents a query, d+d^+d+ is a relevant (positive) document for the query, and d−d^-d− is a",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "non-relevant (negative) document. The model aims to learn representations such that the similarity score between qqq and d+d^+d+ is higher than the score between q and d−d^-d−. The training objective can be mathematically represented as minimizing the following loss function:",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "Loss=max⁡(0,1−S(q,d+)+S(q,d−))\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))Loss=max(0,1−S(q,d+)+S(q,d−)) , where S(q,d)S(q, d)S(q,d) denotes the similarity score computed by ColBERT between a query qqq and a document ddd. This score is obtained by aggregating the max-similarity scores of the",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "best-matching embeddings between the query and the document, following the late interaction pattern described in the model architecture. This approach ensures that the model is trained to distinguish between relevant and irrelevant documents for a given query, by encouraging a larger margin in the",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "similarity scores for positive and negative document pairs. Denoised supervision in ColBERTv2 refines the original training process by selecting challenging negatives and leveraging a cross-encoder for distillation. This sophisticated method of augmenting training data quality involves several",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "steps: Note, this process represents a sophisticated enhancement to the ColBERT training regime rather than a fundamental change to its architecture. The hyperparameters of ColBERT is summarized below: Unlike representation-based approaches that encode each document into one embedding vector,",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "ColBERT encodes documents (and queries) into bags of embeddings, with each token in a document having its own embedding. This approach inherently means that for longer documents, more embeddings will be stored, which is a pain point of  the original ColBERT, and later addressed by ColBERTv2. The key",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "to managing this efficiently lies in ColBERT's use of vector database (e.g. FAISS) for indexing and retrieval, and its detailed indexing process which is designed to handle large volumes of data efficiently. The original ColBERT paper mentions several strategies to enhance the efficiency of indexing",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "and retrieval, including: The introduction of residual compression in ColBERTv2, which is a novel approach not present in the original ColBERT, plays a key role in reducing the model's space footprint by 6–10× while preserving quality. This technique compresses the embeddings further by effectively",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "capturing and storing only the differences from a set of fixed reference centroids.  One might initially assume that incorporating BERT's deep contextual understanding into search would inherently require significant computational resources, making such an approach less feasible for real-time",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "applications due to high latency and computational costs. However, ColBERT challenges and overturns this assumption through its innovative use of the late interaction mechanism. Here are some noteworthy points: Jina-ColBERT is designed for both fast and accurate retrieval, supporting longer context",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "lengths up to 8192, leveraging the advancements of JinaBERT, which allows for longer sequence processing due to its architecture enhancements. Jina-ColBERT's main advancement is its backbone, jina-bert-v2-base-en, which enables processing of significantly longer contexts (up to 8192 tokens) compared",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "to the original ColBERT that uses bert-base-uncased. This capability is crucial for handling documents with extensive content, providing more detailed and contextual search results. We evaluated jina-colbert-v1-en on BEIR datasets and new LoCo benchmark which favors long-context, tested it against",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "the original ColBERTv2 implementation and non-interaction based jina-embeddings-v2-base-en model. This table demonstrates jina-colbert-v1-en's superior  performance, especially in scenarios requiring longer context lengths vs the original ColBERTv2. Note that jina-embeddings-v2-base-en uses more",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "training data, whereas jina-colbert-v1-en only uses MSMARCO, which may justify the good performance of jina-embeddings-v2-base-en on some tasks. This snippet outlines the indexing process with Jina-ColBERT, showcasing its support for long documents. RAGatouille is a new Python library that",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "facilitates the use of advanced retrieval methods within RAG pipelines. It's designed for modularity and easy integration, allowing users to leverage state-of-the-art research seamlessly. The main goal of RAGatouille is to simplify the application of complex models like ColBERT in RAG pipelines,",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "making it accessible for developers to utilize these methods without needing deep expertise in the underlying research. Thanks to Benjamin Clavié, you can now use jina-colbert-v1-en easily: For more detailed information and further exploration of Jina-ColBERT, you can visit the Hugging Face page.",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "ColBERT represents a significant leap forward in the field of information retrieval. By enabling longer context lengths with Jina-ColBERT and maintaining compatibility with the ColBERT approach to late interaction, it offers a powerful alternative for developers looking to implement state-of-the-art",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "search functionality. Coupled with the RAGatouille library, which simplifies the integration of complex retrieval models into RAG pipelines, developers can now harness the power of advanced retrieval with ease, streamlining their workflows and enhancing their applications. The synergy between Jina-",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "ColBERT and RAGatouille illustrates a remarkable stride in making advanced AI search models accessible and efficient for practical use.",
    "url": "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/"
  },
  {
    "text": "Something went wrong. Wait a moment and try again.",
    "url": "https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora"
  }
]